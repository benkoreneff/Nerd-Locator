#!/bin/bash

echo "ğŸš€ Setting up Ollama for LLM-powered tag extraction"
echo "=================================================="

# Check if Ollama is already installed
if command -v ollama &> /dev/null; then
    echo "âœ… Ollama is already installed"
else
    echo "ğŸ“¥ Installing Ollama..."
    curl -fsSL https://ollama.com/install.sh | sh
    
    if [ $? -eq 0 ]; then
        echo "âœ… Ollama installed successfully"
    else
        echo "âŒ Failed to install Ollama"
        exit 1
    fi
fi

# Start Ollama service in background
echo "ğŸ”„ Starting Ollama service..."
ollama serve &
OLLAMA_PID=$!

# Wait for service to start
echo "â³ Waiting for Ollama service to start..."
sleep 5

# Pull the required model
echo "ğŸ“¥ Pulling phi3:mini model (this may take a few minutes)..."
ollama pull phi3:mini

if [ $? -eq 0 ]; then
    echo "âœ… Model phi3:mini pulled successfully"
else
    echo "âŒ Failed to pull model"
    kill $OLLAMA_PID 2>/dev/null
    exit 1
fi

# Test the setup
echo "ğŸ§ª Testing LLM tag extraction..."
cd server
python3 test_llm_tagger.py

if [ $? -eq 0 ]; then
    echo "âœ… LLM tag extraction test passed"
else
    echo "âš ï¸  LLM tag extraction test had issues (this is normal if Ollama isn't running)"
fi

echo ""
echo "ğŸ‰ Setup complete!"
echo ""
echo "To use LLM tag extraction:"
echo "1. Make sure Ollama is running: ollama serve"
echo "2. Start the Civitas application: docker-compose up"
echo "3. Submit civilian profiles with detailed free_text for intelligent tag extraction"
echo ""
echo "Note: If Ollama is not available, the system will automatically fall back to regex-based tagging"
